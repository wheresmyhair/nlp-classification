>>> model
Model(
  (embedding): Embedding(1957, 300, padding_idx=1956)
  (postion_embedding): Positional_Encoding(
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (encoder): Encoder(
    (attention): Multi_Head_Attention(
      (fc_Q): Linear(in_features=300, out_features=300, bias=True)
      (fc_K): Linear(in_features=300, out_features=300, bias=True)
      (fc_V): Linear(in_features=300, out_features=300, bias=True)
      (attention): Scaled_Dot_Product_Attention()
      (fc): Linear(in_features=300, out_features=300, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
    )
    (feed_forward): Position_wise_Feed_Forward(
      (fc1): Linear(in_features=300, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=300, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
    )
  )
  (encoders): ModuleList(
    (0): Encoder(
      (attention): Multi_Head_Attention(
        (fc_Q): Linear(in_features=300, out_features=300, bias=True)
        (fc_K): Linear(in_features=300, out_features=300, bias=True)
        (fc_V): Linear(in_features=300, out_features=300, bias=True)
        (attention): Scaled_Dot_Product_Attention()
        (fc): Linear(in_features=300, out_features=300, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (feed_forward): Position_wise_Feed_Forward(
        (fc1): Linear(in_features=300, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=300, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Encoder(
      (attention): Multi_Head_Attention(
        (fc_Q): Linear(in_features=300, out_features=300, bias=True)
        (fc_K): Linear(in_features=300, out_features=300, bias=True)
        (fc_V): Linear(in_features=300, out_features=300, bias=True)
        (attention): Scaled_Dot_Product_Attention()
        (fc): Linear(in_features=300, out_features=300, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (feed_forward): Position_wise_Feed_Forward(
        (fc1): Linear(in_features=300, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=300, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fc1): Linear(in_features=9600, out_features=2, bias=True)
)


>>> train(config, model, train_iter, dev_iter, test_iter)
Epoch [1/100]
Iter:      0,  Train Loss:  0.52,  Train Acc: 75.78%,  Val Loss:   1.9,                 Val Acc: 78.12%,  Time: 0:00:03 *
Epoch [2/100]
Epoch [3/100]
Epoch [4/100]
Epoch [5/100]
Epoch [6/100]
Epoch [7/100]
Epoch [8/100]
Epoch [9/100]
Epoch [10/100]
Epoch [11/100]
Iter:    100,  Train Loss:  0.39,  Train Acc: 88.28%,  Val Loss:  0.72,                 Val Acc: 58.59%,  Time: 0:00:06 *
Epoch [12/100]
Epoch [13/100]
Epoch [14/100]
Epoch [15/100]
Epoch [16/100]
Epoch [17/100]
Epoch [18/100]
Epoch [19/100]
Epoch [20/100]
Epoch [21/100]
Iter:    200,  Train Loss: 0.049,  Train Acc: 100.00%,  Val Loss:  0.53,                 Val Acc: 78.12%,  Time: 0:00:09 *
Epoch [22/100]
Epoch [23/100]
Epoch [24/100]
Epoch [25/100]
Epoch [26/100]
Epoch [27/100]
Epoch [28/100]
Epoch [29/100]
Epoch [30/100]
Epoch [31/100]
Iter:    300,  Train Loss: 0.032,  Train Acc: 98.44%,  Val Loss:  0.59,                 Val Acc: 83.59%,  Time: 0:00:12
Epoch [32/100]
Epoch [33/100]
Epoch [34/100]
Epoch [35/100]
Epoch [36/100]
Epoch [37/100]
Epoch [38/100]
Epoch [39/100]
Epoch [40/100]
Epoch [41/100]
Iter:    400,  Train Loss: 0.008,  Train Acc: 100.00%,  Val Loss:  0.62,                 Val Acc: 83.59%,  Time: 0:00:16
Epoch [42/100]
Epoch [43/100]
Epoch [44/100]
Epoch [45/100]
Epoch [46/100]
Epoch [47/100]
Epoch [48/100]
Epoch [49/100]
Epoch [50/100]
Epoch [51/100]
Iter:    500,  Train Loss: 0.0041,  Train Acc: 100.00%,  Val Loss:  0.82,                 Val Acc: 83.59%,  Time: 0:00:20
Epoch [52/100]
Epoch [53/100]
Epoch [54/100]
Epoch [55/100]
Epoch [56/100]
Epoch [57/100]
Epoch [58/100]
Epoch [59/100]
Epoch [60/100]
Epoch [61/100]
Iter:    600,  Train Loss: 0.003,  Train Acc: 100.00%,  Val Loss:  0.82,                 Val Acc: 84.38%,  Time: 0:00:23
Epoch [62/100]
Epoch [63/100]
Epoch [64/100]
Epoch [65/100]
Epoch [66/100]
Epoch [67/100]
Epoch [68/100]
Epoch [69/100]
Epoch [70/100]
Epoch [71/100]
Iter:    700,  Train Loss: 0.0013,  Train Acc: 100.00%,  Val Loss:  0.91,                 Val Acc: 82.81%,  Time: 0:00:26
Epoch [72/100]
Epoch [73/100]
Epoch [74/100]
Epoch [75/100]
Epoch [76/100]
Epoch [77/100]
Epoch [78/100]
Epoch [79/100]
Epoch [80/100]
Epoch [81/100]
Iter:    800,  Train Loss: 0.00031,  Train Acc: 100.00%,  Val Loss:  0.67,                 Val Acc: 85.16%,  Time: 0:00:30
Epoch [82/100]
Epoch [83/100]
Epoch [84/100]
Epoch [85/100]
Epoch [86/100]
Epoch [87/100]
Epoch [88/100]
Epoch [89/100]
Epoch [90/100]
Epoch [91/100]
Iter:    900,  Train Loss: 0.0019,  Train Acc: 100.00%,  Val Loss:  0.99,                 Val Acc: 83.59%,  Time: 0:00:33
Epoch [92/100]
Epoch [93/100]
Epoch [94/100]
Epoch [95/100]
Epoch [96/100]
Epoch [97/100]
Epoch [98/100]
Epoch [99/100]
Epoch [100/100]
Test Loss:  0.79,  Test Acc: 71.88%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

          金融     0.9706    0.6600    0.7857       100
          其他     0.4333    0.9286    0.5909        28

    accuracy                         0.7188       128
   macro avg     0.7020    0.7943    0.6883       128
weighted avg     0.8531    0.7188    0.7431       128

Confusion Matrix...
[[66 34]
 [ 2 26]]
Time usage: 0:00:00